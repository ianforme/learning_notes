{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adapted from [Topic Modeling and Latent Dirichlet Allocation (LDA) in Python](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) by Susan Li**\n",
    "\n",
    "**Interesting yet insightful read for LDA from [Intuitive Guide to Latent Dirichlet Allocation\n",
    "](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158) by Thushan Ganegedara, HIGHLY RECOMMENDED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('abcnews-date-text.csv', error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data[['headline_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** : split text into sentence and sentence into words, lowercase with no punctuation\n",
    "\n",
    "**Lemmatization** : 3rd person words, past / future tense words are reduced to their original form\n",
    "\n",
    "**Stemming** : words are reduced to root form, i.e. tokenization -> token (not used in this case)\n",
    "\n",
    "In this case, also removed words with number of characters less than 3 and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def stemming(word):\n",
    "#     return SnowballStemmer(language='english').stem(word)\n",
    "\n",
    "def lemmatize(word, pos):\n",
    "    return WordNetLemmatizer().lemmatize(word=word, pos=pos)\n",
    "\n",
    "def tokenize(sentence):\n",
    "    # note minimum and maximum number of tokens are 2 and 15, default in simple_process\n",
    "    return simple_preprocess(sentence)\n",
    "    \n",
    "def remove_stopwords_shortwords(token_list, min_len):\n",
    "    return [i for i in token_list if i not in STOPWORDS if len(i) > min_len]\n",
    "\n",
    "def process(sentence, pos='v', min_len=3):\n",
    "    tokens = tokenize(sentence)\n",
    "    tokens = remove_stopwords_shortwords(tokens, min_len)\n",
    "    res_tokens = [lemmatize(i, 'v') for i in tokens]\n",
    "    return res_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_text['tokens'] = data_text['headline_text'].map(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "      <td>[decide, community, broadcast, licence]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "      <td>[witness, aware, defamation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "      <td>[call, infrastructure, protection, summit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "      <td>[staff, aust, strike, rise]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "      <td>[strike, affect, australian, travellers]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text  \\\n",
       "0  aba decides against community broadcasting lic...   \n",
       "1     act fire witnesses must be aware of defamation   \n",
       "2     a g calls for infrastructure protection summit   \n",
       "3           air nz staff in aust strike for pay rise   \n",
       "4      air nz strike to affect australian travellers   \n",
       "\n",
       "                                       tokens  \n",
       "0     [decide, community, broadcast, licence]  \n",
       "1                [witness, aware, defamation]  \n",
       "2  [call, infrastructure, protection, summit]  \n",
       "3                 [staff, aust, strike, rise]  \n",
       "4    [strike, affect, australian, travellers]  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dict = gensim.corpora.Dictionary(data_text['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bag of words** : list of (token id, token count[in the document])\n",
    "\n",
    "in gensim, mapping between token and id can be generate through gensim.corpus.Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out words appeared in less than 15 documents \n",
    "# and words with document frequency > 0.5\n",
    "# keep only the first 100000 frequent words\n",
    "text_dict.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bag of words formation for processed tokens\n",
    "bow = [text_dict.doc2bow(doc) for doc in data_text['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1), (5, 1), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (10, 1)],\n",
       " [(11, 1), (12, 1), (13, 1), (14, 1)],\n",
       " [(13, 1), (15, 1), (16, 1), (17, 1)]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of bow formation\n",
    "bow[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**term frequency (tf)** : count number of a token in a document / total length of the document \n",
    "\n",
    "**inverse document frequency (idf)** : log(number of documents / number of appearance of a token in all documents)\n",
    "\n",
    "**tf-idf** : tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the tfidf model using the bag of words\n",
    "tfidf = gensim.models.TfidfModel(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract out tfidf score for these sets of bag of words\n",
    "corpus_tfidf = tfidf[bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.49762407643822143),\n",
       " (1, 0.3948343404468724),\n",
       " (2, 0.5955575073121571),\n",
       " (3, 0.4917187993528625)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [(bow id, tfidf score) ... ]\n",
    "corpus_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation (LDA)\n",
    "LDA is a type of statistical models for discovering the abstract topics that occurs in a collection of documents\n",
    "\n",
    "It builds a topic per document and words per topic model, modeled as Dirichlet distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the lda model from the bag of words\n",
    "# LdaMulticore for parallel processing\n",
    "# This one takes a while to complete...\n",
    "lda_model = gensim.models.LdaMulticore(bow, num_topics=10, id2word=text_dict, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"murder\" + 0.018*\"market\" + 0.018*\"year\" + 0.016*\"australian\" + 0.014*\"attack\" + 0.013*\"record\" + 0.013*\"tasmania\" + 0.013*\"share\" + 0.013*\"sydney\" + 0.013*\"family\"'),\n",
       " (1,\n",
       "  '0.038*\"trump\" + 0.028*\"queensland\" + 0.016*\"rise\" + 0.014*\"price\" + 0.013*\"fall\" + 0.011*\"children\" + 0.009*\"energy\" + 0.009*\"say\" + 0.008*\"inquiry\" + 0.008*\"city\"'),\n",
       " (2,\n",
       "  '0.052*\"police\" + 0.022*\"charge\" + 0.019*\"crash\" + 0.019*\"perth\" + 0.018*\"woman\" + 0.017*\"die\" + 0.015*\"donald\" + 0.014*\"drug\" + 0.013*\"people\" + 0.012*\"jail\"'),\n",
       " (3,\n",
       "  '0.035*\"court\" + 0.022*\"face\" + 0.012*\"sentence\" + 0.012*\"talk\" + 0.011*\"accuse\" + 0.011*\"hold\" + 0.011*\"royal\" + 0.011*\"hear\" + 0.011*\"release\" + 0.010*\"korea\"'),\n",
       " (4,\n",
       "  '0.020*\"government\" + 0.018*\"coast\" + 0.016*\"state\" + 0.015*\"plan\" + 0.014*\"school\" + 0.012*\"fund\" + 0.012*\"gold\" + 0.010*\"say\" + 0.009*\"centre\" + 0.009*\"labor\"'),\n",
       " (5,\n",
       "  '0.024*\"kill\" + 0.017*\"north\" + 0.015*\"arrest\" + 0.015*\"west\" + 0.014*\"south\" + 0.013*\"dead\" + 0.013*\"train\" + 0.012*\"china\" + 0.010*\"island\" + 0.010*\"take\"'),\n",
       " (6,\n",
       "  '0.047*\"australia\" + 0.018*\"test\" + 0.017*\"live\" + 0.016*\"years\" + 0.014*\"tasmanian\" + 0.013*\"turnbull\" + 0.010*\"power\" + 0.009*\"drum\" + 0.009*\"storm\" + 0.008*\"world\"'),\n",
       " (7,\n",
       "  '0.017*\"interview\" + 0.016*\"miss\" + 0.013*\"hour\" + 0.012*\"lose\" + 0.012*\"return\" + 0.010*\"search\" + 0.010*\"beat\" + 0.010*\"game\" + 0.010*\"open\" + 0.010*\"league\"'),\n",
       " (8,\n",
       "  '0.025*\"election\" + 0.017*\"country\" + 0.016*\"rural\" + 0.015*\"hospital\" + 0.013*\"minister\" + 0.012*\"health\" + 0.011*\"win\" + 0.009*\"national\" + 0.009*\"party\" + 0.009*\"port\"'),\n",
       " (9,\n",
       "  '0.014*\"change\" + 0.014*\"indigenous\" + 0.013*\"say\" + 0.010*\"park\" + 0.010*\"water\" + 0.010*\"want\" + 0.010*\"council\" + 0.009*\"service\" + 0.009*\"concern\" + 0.009*\"help\"')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These are ten topics identified by the LDA model\n",
    "# and the words for the model\n",
    "# number means probablities\n",
    "lda_model.print_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LDA models can also be trained from tfidf model\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=text_dict, passes=2, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"rural\" + 0.011*\"news\" + 0.006*\"business\" + 0.006*\"national\" + 0.005*\"cut\" + 0.005*\"fund\" + 0.005*\"care\" + 0.004*\"july\" + 0.004*\"school\" + 0.004*\"centre\"'),\n",
       " (1,\n",
       "  '0.011*\"people\" + 0.010*\"search\" + 0.010*\"miss\" + 0.009*\"weather\" + 0.008*\"queensland\" + 0.006*\"david\" + 0.005*\"police\" + 0.005*\"wild\" + 0.005*\"boat\" + 0.005*\"body\"'),\n",
       " (2,\n",
       "  '0.012*\"market\" + 0.010*\"share\" + 0.007*\"price\" + 0.007*\"australian\" + 0.006*\"christmas\" + 0.006*\"michael\" + 0.006*\"coal\" + 0.005*\"monday\" + 0.005*\"live\" + 0.005*\"dollar\"'),\n",
       " (3,\n",
       "  '0.011*\"turnbull\" + 0.007*\"september\" + 0.007*\"wednesday\" + 0.007*\"peter\" + 0.005*\"grand\" + 0.004*\"wrap\" + 0.004*\"collapse\" + 0.004*\"destroy\" + 0.004*\"blaze\" + 0.004*\"damage\"'),\n",
       " (4,\n",
       "  '0.016*\"interview\" + 0.008*\"crash\" + 0.008*\"john\" + 0.007*\"royal\" + 0.006*\"october\" + 0.006*\"asylum\" + 0.006*\"august\" + 0.005*\"die\" + 0.005*\"june\" + 0.005*\"tony\"'),\n",
       " (5,\n",
       "  '0.010*\"podcast\" + 0.010*\"drum\" + 0.009*\"world\" + 0.009*\"australia\" + 0.008*\"league\" + 0.006*\"street\" + 0.006*\"test\" + 0.006*\"november\" + 0.006*\"ash\" + 0.005*\"final\"'),\n",
       " (6,\n",
       "  '0.012*\"donald\" + 0.010*\"hill\" + 0.008*\"marriage\" + 0.006*\"quiz\" + 0.006*\"april\" + 0.005*\"smith\" + 0.004*\"solar\" + 0.004*\"kill\" + 0.004*\"syrian\" + 0.004*\"say\"'),\n",
       " (7,\n",
       "  '0.018*\"country\" + 0.016*\"hour\" + 0.011*\"government\" + 0.007*\"budget\" + 0.006*\"health\" + 0.006*\"sport\" + 0.006*\"federal\" + 0.005*\"tuesday\" + 0.005*\"mental\" + 0.005*\"december\"'),\n",
       " (8,\n",
       "  '0.017*\"trump\" + 0.017*\"charge\" + 0.015*\"police\" + 0.014*\"murder\" + 0.011*\"court\" + 0.009*\"jail\" + 0.009*\"death\" + 0.009*\"woman\" + 0.008*\"assault\" + 0.008*\"arrest\"'),\n",
       " (9,\n",
       "  '0.008*\"grandstand\" + 0.007*\"rugby\" + 0.005*\"say\" + 0.005*\"korea\" + 0.005*\"abbott\" + 0.005*\"islamic\" + 0.005*\"update\" + 0.005*\"syria\" + 0.005*\"liberal\" + 0.004*\"fan\"')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_tfidf.print_topics() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
